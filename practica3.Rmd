---
title: "Practica3"
author: "Santiago López Valerio"
date: "2025-04-28"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Ejercicio 1
Vamos a trabajar con la base de datos ***data_pca***

```{r, message=FALSE, warning=FALSE}
require(factoextra)
data_pca <- read.csv2("data_pca.csv")
View(data_pca)
```

Ahora, normalizamos los datos:

```{r, message=FALSE, warning=FALSE}
#Normalizar los datos
data_pca1 <- scale(data_pca[,-16])
```

Realizamos el PCA, primero haciendo el diagnostico para el PCA

```{r, message=FALSE, warning=FALSE}
det(cor(data_pca1))  ##si el valor tiende a 0, los datos son adecuados para PCA.
```

EL valor es de 0.004667778, por lo que es adecuado hacer el PCA.
Pasemos a hacer el PCA.

```{r, message=FALSE, warning=FALSE}
#Pasemos a hacer el pca.
pca1 <- princomp(data_pca1)
```

Pasemos al diagnostico

```{r, message=FALSE, warning=FALSE}
summary(pca1)
```

Los primeros 8 componentes son los que explican el 81,17% de la varianza total. 


Revisemos varianza y eigenvalores:

```{r, message=FALSE, warning=FALSE}
#revisar varianza y eigenvalores
fviz_eig(pca1, choice = "variance")
fviz_eig(pca1, choice = "eigenvalue")
```

## NOTA
1. En la primera gráfica, podemos notar que los primeros 6 componentes son los que explican la mayor varianza.

2. En la segunda grafica que explica el eigenvalue de cada componente, podemos notar que los 6 primeros componentes son los que nos brindan mayor informacion.

Vemos que el eigen value es mayor o igual a 1 para los primeros 6, por lo que es correcto.

Ahora veamos el GRAFICO de las puntuaciones factoriales y su representación.
```{r, message=FALSE, warning=FALSE}
fviz_pca_ind(pca1,
             col.ind = "cos2",
             gradient.cols=c("red", "purple", "green"),
             repel = FALSE)
```

Con el grafico anterior, podemos notar que hay demasiados datos que no están bien representados, por lo que el PCA no es adecuado para este conjunto de datos y necesitaremos encontrar una técnica estadística adecuada.

Sigamos con el Grafico de las cargas, lo que nos va a indicar cuanto contribuye cada variable a los diferentes componentes principales. Los componentes contribuyen en diferente medida a los cuadrantes dentro de la representación bidimensional.
```{r, message=FALSE, warning=FALSE}
fviz_pca_var(pca1,
             col.var = "contrib",
             gradient.cols=c("red", "purple", "green"),
             repel = FALSE)
```

Con este gráfico, confirmamos que el PCA no es adecuado para la base de datos. Las flechas rojas indican las pertenecientes a la dimension 1, mientras que las moradas pertenecen a la dimensión 2.

Por último, veamos los dos gráficos anteriores combinados
```{r, message=FALSE, warning=FALSE}
fviz_pca_biplot(pca1,
                col.var = "red",
                col.ind = "blue")
```
```{r, message=FALSE, warning=FALSE}
x11()
psych::cor.plot(data_pca1)
```

Resultados del pca rotando los factores: La rotación más común es varimax. En un pca, los componentes principales iniciales pueden ser difíciles de interpretar porque cada variable puede tener cargas significativas en varios componentes. Al aplicar la rotación varimax, se ajustan las cargas de manera 
que cada variable tenga una carga alta en un solo componente, haciendo que la estructura sea más simple y clara.

```{r, message=FALSE, warning=FALSE}
pca2 <- psych::principal(data_pca1,nfactors = 2,residuals = FALSE,rotate = "varimax",
                         scores = TRUE,oblique.scores = FALSE,method = "regression",
                         use = "pairwise",cor = "cor",weight = NULL)

pca2
```

Ahora, la matriz de coeficientes para las puntuaciones de los componentes.
```{r, message=FALSE, warning=FALSE}
pca2$weights[,1]
pca2$weights[,2]
```
```{r, message=FALSE, warning=FALSE}
pca2$scores
```


##Ejercicio 2
```{r, message=FALSE, warning=FALSE}
require(readxl)
getwd()
poblacionUSA <- read_xlsx("PoblacionUSA.xlsm")
poblacionUSA_2000 <- poblacionUSA[, c(1,2,3,5,7,9,11,13,15,17,19)]
poblacionUSA_2001 <- poblacionUSA[, c(1,4,6,8,10,12,14,16,18,20)]
```

Lo que hicimos, dividir la base de datos ***poblacionUSA*** para el año 2000 y 2001.

Vamos a normalizar los datos.

```{r, message=FALSE, warning=FALSE}
data_pca2 <- scale(poblacionUSA_2000[,-1])
data_pca3 <- scale(poblacionUSA_2001[,-1])
```

Realizamos el PCA para el año 2000. Primero un diagnóstico para el PCA

```{r, message=FALSE, warning=FALSE}
det(cor(data_pca2)) 
```

El valor que nos arroja tiende 0, por lo que los datos son adecuados para PCA. Por lo que ahora procedemos a calcular el factor de adecuación muestral de Kaiser.

```{r, message=FALSE, warning=FALSE}
psych::KMO(data_pca2) 

```

El valor del MSA es de 0.5, por lo que los datos son mediocres pero útiles para PCA. Lo ideal sería que fuera mayor a 0.6.
#Procedamos a hacer el pca para el año 2000.

```{r, message=FALSE, warning=FALSE}
pca_2000 <- princomp(data_pca2)
```

Vamos a hacer el diagnostico correspondiente:

```{r, message=FALSE, warning=FALSE}
summary(pca_2000)
```

Los principales componentes que aportan mayor varianza son el 1 y 2 con el 87.5%. Vamos a revisar la varianza y eigenvalues.

```{r, message=FALSE, warning=FALSE}
fviz_eig(pca_2000, choice = "variance")
fviz_eig(pca_2000, choice = "eigenvalue")
```

##NOTA
Podemos notar con estos dos gráficos, que efectivamente los dos primeros componentes son los más importantes dentro del análisis.

Sigamos con el PCA para el año 2001 mediante su diagnóstico.

```{r, message=FALSE, warning=FALSE}
det(cor(data_pca3)) 
```

El valor que nos arroja tiende 0, por lo que los datos son adecuados para PCA. Calculemos factor de adecuación muestral de Kaiser.

```{r, message=FALSE, warning=FALSE}
psych::KMO(data_pca2) 
```

El valor del MSA es de 0.5, por lo que los datos son mediocres pero útiles para PCA. Lo ideal sería que fuera mayor a 0.6. Procedamos a hacer el pca para el año 2001.

```{r, message=FALSE, warning=FALSE}
pca_2001 <- princomp(data_pca3)
```

El diagnostico nos va a arrojar lo siguiente:

```{r, message=FALSE, warning=FALSE}
summary(pca_2001)
```

Los principales componentes que aportan mayor varianza son el 1 y 2 con el 89.84%. Revisemos varianza y eigenvalores.

```{r, message=FALSE, warning=FALSE}
fviz_eig(pca_2001, choice = "variance")
fviz_eig(pca_2001, choice = "eigenvalue")
```

Podemos notar con estos dos gráficos, que efectivamente los dos primeros componentes son los más importantes dentro del análisis.


Vamos a realizar el GRAFICO de las puntuaciones factoriales y su representación para el año 2000.

```{r, message=FALSE, warning=FALSE}
fviz_pca_ind(pca_2000,
             col.ind = "cos2",
             gradient.cols=c("red", "pink", "purple"),
             repel = FALSE)

```

Podemos observar que son pocas observaciones las que están mal representadas, como por ejemplo el dato 26.

Ahora hagamos el GRAFICO de las puntuaciones factoriales y su representación para el año 2001.

```{r, message=FALSE, warning=FALSE}
fviz_pca_ind(pca_2001,
             col.ind = "cos2",
             gradient.cols=c("red", "pink", "purple"),
             repel = FALSE)
```

Podemos observar que son pocas observaciones las que están mal representadas, como por ejemplo el dato 21.}

Con el Grafico de las cargas para el año 2000 nos indicará en cuanto contribuye cada variable a los diferentes componentes principales. Los componentes contribuyen en diferente medida a los cuadrantes dentro de la representación bidimensional.

```{r, message=FALSE, warning=FALSE}
fviz_pca_var(pca_2000,
             col.var = "contrib",
             gradient.cols=c("red", "pink", "purple"),
             repel = FALSE)
```

Podemos notar que tres de las variables están representadas en la dimensión 1 y las demás en dimensión 2.

Ahora bien, el gráfico de las cargas para el año 2001.

```{r, message=FALSE, warning=FALSE}
fviz_pca_var(pca_2001,
             col.var = "contrib",
             gradient.cols=c("red", "pink", "purple"),
             repel = FALSE)
```

De igual manera son tres las variables que están bien representadas en la dimensión 1, las demás están en la dimensión 2.

Ahora vamos a generar el gráfico combinado de los dos anteriores, en este caso para el año 2000:
```{r, message=FALSE, warning=FALSE}
fviz_pca_biplot(pca_2000,
                col.var = "red",
                col.ind = "blue")
```

Ahora para el año 2001:
```{r, message=FALSE, warning=FALSE}

fviz_pca_biplot(pca_2001,
                col.var = "red",
                col.ind = "blue")
```

```{r, message=FALSE, warning=FALSE}
x11()
psych::cor.plot(data_pca2)

x11()
psych::cor.plot(data_pca3)
```

Resultados del pca rotando los factores: La rotación más común es varimax. En un pca, los componentes principales iniciales pueden ser difíciles de interpretar porque cada variable puede tener cargas significativas en varios componentes. Al aplicar la rotación varimax, se ajustan las cargas de manera que cada variable tenga una carga alta en un solo componente, haciendo que la estructura sea más simple y clara.

```{r, message=FALSE, warning=FALSE}
pca2000 <- psych::principal(data_pca2,nfactors = 2,residuals = FALSE,rotate = "varimax",
                         scores = TRUE,oblique.scores = FALSE,method = "regression",
                         use = "pairwise",cor = "cor",weight = NULL)
pca2000


pca2001 <- psych::principal(data_pca3,nfactors = 2,residuals = FALSE,rotate = "varimax",
                            scores = TRUE,oblique.scores = FALSE,method = "regression",
                            use = "pairwise",cor = "cor",weight = NULL)
pca2001
```

Ahora vamos a representar la Matriz de coeficientes para las puntuaciones de los componentes de cada año.
Para el año 2000:

```{r, message=FALSE, warning=FALSE}
pca2000$weights[,1]
pca2000$weights[,2]
```

Para el año 2001:
```{r, message=FALSE, warning=FALSE}
pca2001$weights[,1]
pca2001$weights[,2]
```

```{r, message=FALSE, warning=FALSE}
pca2000$scores
pca2001$scores
```


